
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Multi-step Moral Dilemmas</title>
  <link rel="icon" type="image/x-icon" href="static/images/mmds_icon.webp">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      
      <!-- Title -->
      <h1 id="MMDs" class="title is-3 publication-title">
        The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas
      </h1>

      <!-- Buttons -->
      <div class="publication-links mt-4">
        <span class="link-block">
          <a href="your-paper-link.pdf" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-file-pdf"></i>
            </span>
            <span>Paper</span>
          </a>
        </span>

        <span class="link-block">
          <a href="https://arxiv.org/pdf/2504.20013" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>

        <span class="link-block">
          <a href="static/pdf/MMDs.zip" class="external-link button is-normal is-rounded is-dark" download>
            <span class="icon">
              <i class="fas fa-download"></i>
            </span>
            <span>Download Dataset (.zip)</span>
          </a>
        </span>
      </div>

    </div>
  </div>
</section>


  <section class="section">

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
              <figure>
                <img src="static/images/motivation.jpg" style="width:110%" alt="Figure 0">
              </figure>
          </div>
        </div>
      </div> 
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Ethical decision-making is a critical aspect of human judgment, and the growing use of LLMs in decision-support systems necessitates a rigorous evaluation of their moral reasoning capabilities.  However, existing assessments primarily rely on single-step evaluations, failing to capture how models adapt to evolving ethical challenges. Addressing this gap, we introduce the Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas. This framework enables a fine-grained, dynamic analysis of how LLMs adjust their moral reasoning across escalating dilemmas.  Our evaluation of nine widely used LLMs reveals that their value preferences shift significantly as dilemmas progress, indicating that models recalibrate moral judgments based on scenario complexity.
Furthermore, pairwise value comparisons demonstrate that while LLMs often prioritize the value of care, this value can sometimes be superseded by fairness in certain contexts, highlighting the dynamic and context-dependent nature of LLM ethical reasoning.
Our findings call for a shift toward dynamic, context-aware evaluation paradigms, paving the way for more human-aligned and value-sensitive development of LLMs.</p>
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <div style="text-align:center">
        <h2 class="title is-3">Dataset Construction</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>Using GPT-4o, we generate structured dilemmas with five escalating steps (M = {S<sub>1</sub>,S<sub>2</sub>,...,S<sub>5</sub>}), where each step S<sub>i</sub> (i ∈ {1,2,...,5}) is defined as:</p>
<p>The framework progresses through:</p>

<ol>
  <li><strong>Step 1</strong>: Introduces a core moral conflict between two foundational values. For example, should one intervene to prevent immediate harm (potentially through violence), or avoid action and risk more severe outcomes?</li>
  <li><strong>Steps 2-4</strong>: Gradually increase complexity by layering additional, intersecting moral tensions. Rather than altering surface context, these steps introduce new value conflicts, e.g., loyalty versus harm reduction, or justice versus security transforming the dilemma from a binary trade-off into a multi-dimensional ethical problem.</li>
  <li><strong>Step 5</strong>: Presents a high-stakes scenario requiring the model to navigate deeply conflicting principles. For instance, the model may face a choice between violating core ethical norms (e.g., using torture) for a perceived greater good, or maintaining moral integrity at significant cost.</li>
</ol>
      </div>
      <div class="columns is-centered">
          <figure>
            <img src="static/images/framework.jpg" alt="Figure 1" style="width:110%; display:block; margin:0 auto;">
          </figure>
      </div>
      <br>
      <br>
<div style="text-align:center">
<h2 class="title is-3">Consensus-Based Model Value Mapping</h2>
</div>
<p>To label each action A<sub>i</sub> and B<sub>i</sub> in step S<sub>i</sub> with moral value dimensions, we adopt two established frameworks: Moral Foundations Theory (MFT) and Schwartz’s Basic Human Values.</p>

<p>Each step receives a value pair (V<sub>i</sub><sup>A</sup>, V<sub>i</sub><sup>B</sup>), ensuring distinct moral dimensions. To reduce bias and improve reliability, we use a consensus-based annotation process with three LLMs: GPT-4o-mini, GLM-4-Plus, and DeepSeek-V3.</p>

<ul>
  <li><strong>Value Recognition</strong>: Each model independently assigns values using Chain-of-Thought reasoning from a first-person stakeholder view, based on MFT or Schwartz.</li>
  <li><strong>Consistency Check</strong>: If two models agree, we adopt the majority label; otherwise, human adjudicators resolve disagreements.</li>
</ul>

      
 <div style="text-align:center">
          <figure>
            <img src="static/images/consensus_dataset.png" alt="Figure 1" style="width:80%; display:block; margin:0 auto;">
          </figure>
      </div>
  <br>
  <br>
<div style="text-align:center">
<h2 class="title is-3">Evaluation Result Analysis</h2>
</div>
      <div style = "text-align:center">
      <p>To assess the value preferences of LLMs in dynamic moral dilemmas, we evaluated nine mainstream models, including DeepSeek-V3, GPT-4o, LLaMA-3-70B, GLM-4 (Air-0111 and Plus), Qwen-Plus, Mistral-Small-24B-Instruct-2501, Gemini-2.0-Flash, and Claude-3-5-Haiku—using our MMDs dataset. Our experimental design incorporates history-aware reasoning to simulate human-like moral dynamics, grounded in cumulative moral development theory.</p>  
      </div>
      <!-- Finding 1 -->
      <div style="text-align:center">
        <h2 class="title is-4">Finding 1: LLMs maintain value orientations while flexibly adjusting preference strengths across dilemmas.</h2>
        <div class="content has-text-justified">
         <p>LLMs exhibit stable value directions while adjusting preference intensity. Using preference scores from -0.5 to +0.5, we find that models retain consistent positive/negative directions across five steps. Value priority is generally stable: care > fairness > sanctity > authority > liberty > loyalty. As dilemmas deepen, models adjust intensities: fairness rises (e.g., Gemini: +0.026 to +0.182), aversion to liberty weakens (GPT-4o: -0.232 to -0.164), and loyalty is more strongly rejected (GLM-4-Air: -0.232 to -0.314). Sanctity fluctuates most, often reversing direction (Claude: +0.02 to -0.083), while care remains remarkably stable (+0.13 to +0.24), suggesting harm prevention is a moral anchor.</p>
          </div>
        <div style="text-align:center">
        <figure>
          <img src="static/images/intra_mft.png" alt="Finding 1 illustration" style="width:100%;display:block;margin:0 auto;">
        </figure>
          </div>
      </div>
<br>
  <br>
      <!-- Finding 2 -->
     <div style="text-align:center">
        <h2 class="title is-4">Finding 2: Model preferences evolve dynamically with varying stability across dimensions.</h2>
       <div class="content has-text-justified"> 
       <p>We assess inter-model stability using Spearman’s rank correlation across reasoning steps in six moral dimensions. Liberty shows the highest and most consistent agreement, indicating strong convergence on autonomy-related judgments. Care and sanctity also display increasing consistency over time. In contrast, authority shows a marked decline in consistency, suggesting growing divergence among models. Fairness remains unstable, with models varying in its relative importance. Loyalty shows delayed but increasing agreement, reflecting gradual alignment under more intense dilemmas.</p>
         <p>Based on ranking shifts, models can be grouped into three types. Highly volatile models (like Llama, Gemini, and DeepSeek) frequently change rankings across dimensions. Adaptive models (such as Qwen-plus and GLM-4-Plus) make targeted adjustments while maintaining overall stability. Stable models (Claude and GLM-4-Air) retain consistent value rankings across all steps. A similar pattern is observed under Schwartz’s value framework.</p>
       </div>
       <div style="text-align:center">
         <figure>
          <img src="static/images/inter_mft.png" alt="Finding 2 illustration" style="width:100%;display:block;margin:0 auto;">
        </figure>
         </div>
      </div>
      <br>
      <br>
      <!-- Finding 3 -->
      <div style="text-align:center">
        <h2 class="title is-4">Finding 3: LLMs do not rely on stable moral principles, but rather generate value preferences through
context-driven statistical imitation.</h2>
        <div class="content has-text-justified"> 
        <p>We analyzed the preference structures of large language models when facing moral dilemmas and found that while most models exhibit a degree of overall consistency, local intransitivities frequently emerge. For example, in DeepSeek, GPT-4o, and GLM-4-Air, models tend to prefer care over sanctity and sanctity over fairness, yet show nearly equal preference between care and fairness (win rates around 0.52–0.54). This creates a locally intransitive cycle that prevents a clear value hierarchy. Such patterns suggest that the models’ moral judgments are not guided by a stable internal value system, but are more likely shaped by context-driven trade-offs. The recurrence of these intransitivities across models highlights a fundamental limitation: current LLMs lack consistent and rational value orderings when navigating ethical conflicts.
</p>
          </div>
        <div style="text-align:center">
        <figure>
          <img src="static/images/winrate_mft_1.jpg" alt="Finding 3 trade-off matrix" style="width:100%;display:block;margin:0 auto;">
        </figure>
          </div>
      </div>

      

    
      <!--/ Method -->
        </div>
      </div>
</section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              The primary code for this webpage is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a> and <a href="https://github.com/llm-misinformation/llm-misinformation.github.io/">LLM-Misinformation</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</html>
