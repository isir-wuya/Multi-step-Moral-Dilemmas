
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Multi-step Moral Dilemmas</title>
  <link rel="icon" type="image/x-icon" href="static/images/mmds_icon.webp">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      
      <!-- Title -->
      <h1 id="MMDs" class="title is-3 publication-title">
        The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas
      </h1>

      <!-- Buttons -->
      <div class="publication-links mt-4">
        <span class="link-block">
          <a href="your-paper-link.pdf" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-file-pdf"></i>
            </span>
            <span>Paper</span>
          </a>
        </span>

        <span class="link-block">
          <a href="https://arxiv.org/pdf/2504.20013" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>

        <span class="link-block">
          <a href="assets/dataset.zip" class="external-link button is-normal is-rounded is-dark" download>
            <span class="icon">
              <i class="fas fa-download"></i>
            </span>
            <span>Download Dataset (.zip)</span>
          </a>
        </span>
      </div>

    </div>
  </div>
</section>


  <section class="section">

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
              <figure>
                <img src="static/images/motivation.jpg" style="width:110%" alt="Figure 0">
              </figure>
          </div>
        </div>
      </div> 
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Ethical decision-making is a critical aspect of human judgment, and the growing use of LLMs in decision-support systems necessitates a rigorous evaluation of their moral reasoning capabilities.  However, existing assessments primarily rely on single-step evaluations, failing to capture how models adapt to evolving ethical challenges. Addressing this gap, we introduce the Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas. This framework enables a fine-grained, dynamic analysis of how LLMs adjust their moral reasoning across escalating dilemmas.  Our evaluation of nine widely used LLMs reveals that their value preferences shift significantly as dilemmas progress, indicating that models recalibrate moral judgments based on scenario complexity.
Furthermore, pairwise value comparisons demonstrate that while LLMs often prioritize the value of care, this value can sometimes be superseded by fairness in certain contexts, highlighting the dynamic and context-dependent nature of LLM ethical reasoning.
Our findings call for a shift toward dynamic, context-aware evaluation paradigms, paving the way for more human-aligned and value-sensitive development of LLMs.</p>
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <div style="text-align:center">
        <h2 class="title is-3">Dataset Construction</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>Using GPT-4o, we generate structured dilemmas with five escalating steps (M = {S<sub>1</sub>,S<sub>2</sub>,...,S<sub>5</sub>}), where each step S<sub>i</sub> (i ∈ {1,2,...,5}) is defined as:</p>

<p>The framework progresses through:</p>

<ol>
  <li><strong>Step 1</strong>: Introduces a core moral conflict between two foundational values. For example, should one intervene to prevent immediate harm (potentially through violence), or avoid action and risk more severe outcomes?</li>
  <li><strong>Steps 2-4</strong>: Gradually increase complexity by layering additional, intersecting moral tensions. Rather than altering surface context, these steps introduce new value conflicts, e.g., loyalty versus harm reduction, or justice versus security transforming the dilemma from a binary trade-off into a multi-dimensional ethical problem.</li>
  <li><strong>Step 5</strong>: Presents a high-stakes scenario requiring the model to navigate deeply conflicting principles. For instance, the model may face a choice between violating core ethical norms (e.g., using torture) for a perceived greater good, or maintaining moral integrity at significant cost.</li>
</ol>
      </div>
      <div class="columns is-centered">
          <figure>
            <img src="static/images/framework.jpg" alt="Figure 1" style="width:110%; display:block; margin:0 auto;">
          </figure>
      </div>
      <br>
      <br>

  <div style="text-align:center">
    <h2 class="title is-3">Dataset Construction</h2>
  </div>
  <div class="content has-text-justified">
    <br>
    <p>Using GPT-4o, we generate structured dilemmas with five escalating steps (M = {S<sub>1</sub>, S<sub>2</sub>, ..., S<sub>5</sub>}), where each step S<sub>i</sub> (i ∈ {1,2,...,5}) is defined as:</p>

    <p>The framework progresses through:</p>
    <ol>
      <li><strong>Step 1</strong>: Introduces a core moral conflict between two foundational values. For example, should one intervene to prevent immediate harm (potentially through violence), or avoid action and risk more severe outcomes?</li>
      <li><strong>Steps 2–4</strong>: Gradually increase complexity by layering additional, intersecting moral tensions. Rather than altering surface context, these steps introduce new value conflicts, e.g., loyalty versus harm reduction, or justice versus security, transforming the dilemma from a binary trade-off into a multi-dimensional ethical problem.</li>
      <li><strong>Step 5</strong>: Presents a high-stakes scenario requiring the model to navigate deeply conflicting principles. For instance, the model may face a choice between violating core ethical norms (e.g., using torture) for a perceived greater good, or maintaining moral integrity at significant cost.</li>
    </ol>

    <br>
    <h4 class="title is-4">Consensus-Based Model Value Mapping</h4>
    <p>To assign moral value dimensions to each action A<sub>i</sub> and B<sub>i</sub> in every step S<sub>i</sub>, we leverage two well-established moral frameworks: Moral Foundations Theory (MFT) and Schwartz’s Theory of Basic Human Values. Definitions and interpretations of all value dimensions are provided in Appendix A.</p>

    <p>For each step S<sub>i</sub>, we determine a pair of value annotations V<sub>i</sub><sup>A</sup> and V<sub>i</sub><sup>B</sup>, ensuring that the two values correspond to distinct moral dimensions within the selected framework. To mitigate biases arising from single-model annotations and enhance the reliability of value attribution, we employ a consensus-based approach using three LLMs: GPT-4o-mini, GLM-4-Plus, and DeepSeek-V3.<sup><a href="#human_verification">1</a></sup></p>

    <p>The value mapping process proceeds as follows:</p>
    <ul>
      <li><strong>Value Recognition</strong>: Each model independently maps the candidate actions A<sub>i</sub> and B<sub>i</sub> to their respective value dimensions, V<sub>i</sub><sup>A</sup> and V<sub>i</sub><sup>B</sup>. We prompt the models to use Chain-of-Thought (CoT) reasoning, encouraging them to analyze each decision from a first-person stakeholder perspective. This aligns with stakeholder-centric approaches discussed in prior work. Specifically, the models are required to articulate the expected consequences of each action, identify impacted stakeholders, and justify the associated moral value based on MFT or Schwartz’s value definitions.</li>

      <li><strong>Consistency Check</strong>: If at least two out of the three models agree on the value assignment for an action, we adopt that value. In cases where all three models produce divergent labels, manual adjudication by human annotators is used to determine the most appropriate classification.</li>

      <li><strong>Final Structure</strong>: After assigning values, each moral dilemma step is formally represented as a tuple including the context, dilemma, two actions, and their corresponding values:  
        <br><code>S<sub>i</sub> = (Ctx<sub>i</sub>, D<sub>i</sub>, A<sub>i</sub>, B<sub>i</sub>, V<sub>i</sub><sup>A</sup>, V<sub>i</sub><sup>B</sup>)</code>,  
        ensuring that V<sub>i</sub><sup>A</sup> ≠ V<sub>i</sub><sup>B</sup> and both are valid within the target moral framework.</li>
    </ul>

    <p>The statistics of the resulting dataset are shown in Table 1. <strong><em>Care</em></strong> and <strong><em>Benevolence</em></strong> are the most frequently assigned values across all LLMs, while <strong><em>Sanctity</em></strong>, <strong><em>Tradition</em></strong>, and <strong><em>Stimulation</em></strong> are least represented. Additionally, <strong><em>Liberty</em></strong>, <strong><em>Security</em></strong>, and <strong><em>Power</em></strong> exhibit notable judgment variation across different LLMs.</p>
  </div>

  <div class="columns is-centered">
    <figure>
      <img src="static/images/framework.jpg" alt="Figure 1" style="width:110%; display:block; margin:0 auto;">
    </figure>
  </div>
  <br>
  <br>

      <!-- Finding 1 -->
      <div style="text-align:center">
        <h2 class="title is-4">Finding 1: LLMs maintain value orientations while flexibly adjusting preference strengths across dilemmas.</h2>
        <div class="content has-text-justified">
         <p>LLMs exhibit stable value directions while adjusting preference intensity. Using preference scores from -0.5 to +0.5, we find that models retain consistent positive/negative directions across five steps. Value priority is generally stable: care > fairness > sanctity > authority > liberty > loyalty. As dilemmas deepen, models adjust intensities: fairness rises (e.g., Gemini: +0.026 to +0.182), aversion to liberty weakens (GPT-4o: -0.232 to -0.164), and loyalty is more strongly rejected (GLM-4-Air: -0.232 to -0.314). Sanctity fluctuates most, often reversing direction (Claude: +0.02 to -0.083), while care remains remarkably stable (+0.13 to +0.24), suggesting harm prevention is a moral anchor.</p>
          </div>
        <div style="text-align:center">
        <figure>
          <img src="static/images/intra_mft.png" alt="Finding 1 illustration" style="width:100%;display:block;margin:0 auto;">
        </figure>
          </div>
      </div>

      <!-- Finding 2 -->
     <div style="text-align:center">
        <h2 class="title is-4">Finding 2: Model preferences evolve dynamically with varying stability across dimensions.</h2>
       <div class="content has-text-justified"> 
       <p>We assess inter-model stability using Spearman’s rank correlation across reasoning steps in six moral dimensions. Liberty shows the highest and most consistent agreement, indicating strong convergence on autonomy-related judgments. Care and sanctity also display increasing consistency over time. In contrast, authority shows a marked decline in consistency, suggesting growing divergence among models. Fairness remains unstable, with models varying in its relative importance. Loyalty shows delayed but increasing agreement, reflecting gradual alignment under more intense dilemmas.
         Based on ranking shifts, models can be grouped into three types. Highly volatile models (like Llama, Gemini, and DeepSeek) frequently change rankings across dimensions. Adaptive models (such as Qwen-plus and GLM-4-Plus) make targeted adjustments while maintaining overall stability. Stable models (Claude and GLM-4-Air) retain consistent value rankings across all steps. A similar pattern is observed under Schwartz’s value framework.</p>
       </div>
       <div style="text-align:center">
         <figure>
          <img src="static/images/inter_mft.png" alt="Finding 2 illustration" style="width:100%;display:block;margin:0 auto;">
        </figure>
         </div>
      </div>

      <!-- Finding 3 -->
      <div style="text-align:center">
        <h2 class="title is-4">Finding 3: LLMs do not rely on stable moral principles, but rather generate value preferences through
context-driven statistical imitation.</h2>
        <div class="content has-text-justified"> 
        <p>We analyzed the preference structures of large language models when facing moral dilemmas and found that while most models exhibit a degree of overall consistency, local intransitivities frequently emerge. For example, in DeepSeek, GPT-4o, and GLM-4-Air, models tend to prefer *care* over *sanctity* and *sanctity* over *fairness*, yet show nearly equal preference between *care* and *fairness* (win rates around 0.52–0.54). This creates a locally intransitive cycle that prevents a clear value hierarchy. Such patterns suggest that the models’ moral judgments are not guided by a stable internal value system, but are more likely shaped by context-driven trade-offs. The recurrence of these intransitivities across models highlights a fundamental limitation: current LLMs lack consistent and rational value orderings when navigating ethical conflicts.
</p>
          </div>
        <div style="text-align:center">
        <figure>
          <img src="static/images/winrate_mft_1.jpg" alt="Finding 3 trade-off matrix" style="width:100%;display:block;margin:0 auto;">
        </figure>
          </div>
      </div>

      

    
      <!--/ Method -->
        </div>
      </div>
</section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              The primary code for this webpage is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a> and <a href="https://github.com/llm-misinformation/llm-misinformation.github.io/">LLM-Misinformation</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</html>
